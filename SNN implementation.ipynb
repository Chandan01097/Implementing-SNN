{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a7ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- STAGE 1: Building and Training Beat Classifier (Model A) on MIT-BIH Data ---\n",
      "MIT-BIH dataset loaded successfully.\n",
      "Full MIT shape: (109446, 188)\n",
      "Model A Epoch 1/15, Train Loss: 0.7784\n",
      "Model A Val Accuracy: 0.8277\n",
      "Model A Epoch 2/15, Train Loss: 0.7216\n",
      "Model A Val Accuracy: 0.8276\n",
      "Model A Epoch 3/15, Train Loss: 0.7301\n",
      "Model A Val Accuracy: 0.8277\n",
      "Model A Epoch 4/15, Train Loss: 0.7115\n",
      "Model A Val Accuracy: 0.8277\n",
      "Model A Epoch 5/15, Train Loss: 0.7164\n",
      "Model A Val Accuracy: 0.8277\n",
      "Model A Epoch 6/15, Train Loss: 0.6928\n",
      "Model A Val Accuracy: 0.8288\n",
      "Model A Epoch 7/15, Train Loss: 0.6699\n",
      "Model A Val Accuracy: 0.8271\n",
      "Model A Epoch 8/15, Train Loss: 0.6535\n",
      "Model A Val Accuracy: 0.8254\n",
      "Model A Epoch 9/15, Train Loss: 0.6252\n",
      "Model A Val Accuracy: 0.8514\n",
      "Model A Epoch 10/15, Train Loss: 0.5848\n",
      "Model A Val Accuracy: 0.8513\n",
      "Model A Epoch 11/15, Train Loss: 0.5547\n",
      "Model A Val Accuracy: 0.8410\n",
      "Model A Epoch 12/15, Train Loss: 0.5486\n",
      "Model A Val Accuracy: 0.8488\n",
      "Model A Epoch 13/15, Train Loss: 0.5084\n",
      "Model A Val Accuracy: 0.8304\n",
      "Model A Epoch 14/15, Train Loss: 0.5114\n",
      "Model A Val Accuracy: 0.8875\n",
      "Model A Epoch 15/15, Train Loss: 0.4976\n",
      "Model A Val Accuracy: 0.8760\n",
      "Stage 1 Complete: Beat Classifier (Model A) Trained.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.88      0.99      0.93     18118\n",
      "           S       0.00      0.00      0.00       556\n",
      "           V       0.97      0.22      0.36      1447\n",
      "           F       0.00      0.00      0.00       161\n",
      "           Q       0.78      0.56      0.65      1608\n",
      "\n",
      "    accuracy                           0.88     21890\n",
      "   macro avg       0.53      0.35      0.39     21890\n",
      "weighted avg       0.85      0.88      0.84     21890\n",
      "\n",
      "\n",
      "--- STAGE 2: Building and Training Anomaly Detector (Model B) on PTBDB Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTBDB dataset loaded successfully.\n",
      "Full PTBDB shape: (14552, 188)\n",
      "Model B Epoch 1/15, Train Loss: 0.8914\n",
      "Model B Val Accuracy: 0.5373\n",
      "Model B Epoch 2/15, Train Loss: 0.6149\n",
      "Model B Val Accuracy: 0.4253\n",
      "Model B Epoch 3/15, Train Loss: 0.6070\n",
      "Model B Val Accuracy: 0.5070\n",
      "Model B Epoch 4/15, Train Loss: 0.5902\n",
      "Model B Val Accuracy: 0.7176\n",
      "Model B Epoch 5/15, Train Loss: 0.5723\n",
      "Model B Val Accuracy: 0.6788\n",
      "Model B Epoch 6/15, Train Loss: 0.5718\n",
      "Model B Val Accuracy: 0.7224\n",
      "Model B Epoch 7/15, Train Loss: 0.5679\n",
      "Model B Val Accuracy: 0.7217\n",
      "Model B Epoch 8/15, Train Loss: 0.5692\n",
      "Model B Val Accuracy: 0.4885\n",
      "Model B Epoch 9/15, Train Loss: 0.5660\n",
      "Model B Val Accuracy: 0.6853\n",
      "Model B Epoch 10/15, Train Loss: 0.5616\n",
      "Model B Val Accuracy: 0.6482\n",
      "Model B Epoch 11/15, Train Loss: 0.5643\n",
      "Model B Val Accuracy: 0.5019\n",
      "Model B Epoch 12/15, Train Loss: 0.5459\n",
      "Model B Val Accuracy: 0.5854\n",
      "Model B Epoch 13/15, Train Loss: 0.5669\n",
      "Model B Val Accuracy: 0.7272\n",
      "Model B Epoch 14/15, Train Loss: 0.5401\n",
      "Model B Val Accuracy: 0.7248\n",
      "Model B Epoch 15/15, Train Loss: 0.5490\n",
      "Model B Val Accuracy: 0.5620\n",
      "Stage 2 Complete: Anomaly Detector (Model B) Trained.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.38      0.88      0.53       809\n",
      "    Abnormal       0.90      0.44      0.59      2102\n",
      "\n",
      "    accuracy                           0.56      2911\n",
      "   macro avg       0.64      0.66      0.56      2911\n",
      "weighted avg       0.76      0.56      0.57      2911\n",
      "\n",
      "\n",
      "--- Inference on Original MIT-BIH Test Split using Model A ---\n",
      "Sample MIT Test Predictions (first 10): [np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "\n",
      "All Done! Models trained on respective datasets. For cascaded use: Apply Model B first to detect anomaly, then Model A for beat details if anomalous.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# # Set your local folder path where the CSV files are stored\n",
    "# local_folder = r\"ENTER_YOUR_EXISTING_FOLDER_PATH_HERE\"  # <<<--- CHANGE THIS, e.g., r\"C:\\Users\\saura\\Downloads\\heartbeat\"\n",
    "# os.chdir(local_folder)  # Change working directory to make relative loads work\n",
    "# print(\"Working in directory:\", os.getcwd())\n",
    "# print(\"Files available:\", os.listdir())\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# SNN Components (unchanged)\n",
    "class SpikeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mem, threshold):\n",
    "        spike = (mem >= threshold).float()\n",
    "        ctx.save_for_backward(mem - threshold)\n",
    "        return spike\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        mem_minus_th, = ctx.saved_tensors\n",
    "        surrogate = 1 / (1 + (torch.abs(mem_minus_th) * 5) ** 2)\n",
    "        return grad_output * surrogate, None\n",
    "\n",
    "class LIF(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold=1.0, decay=0.99):\n",
    "        super(LIF, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.threshold = threshold\n",
    "        self.decay = decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        time_steps = x.size(1)\n",
    "        mem = torch.zeros(batch_size, self.fc.out_features, device=x.device)\n",
    "        spikes = []\n",
    "        for t in range(time_steps):\n",
    "            curr = self.fc(x[:, t, :])\n",
    "            mem = self.decay * mem + curr\n",
    "            spike = SpikeFunction.apply(mem, self.threshold)\n",
    "            mem = mem - spike * self.threshold\n",
    "            spikes.append(spike)\n",
    "        return torch.stack(spikes, dim=1)\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__()\n",
    "        self.lif1 = LIF(input_size, hidden_size)\n",
    "        self.lif2 = LIF(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spk1 = self.lif1(x)\n",
    "        spk2 = self.lif2(spk1)\n",
    "        return spk2\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, signals, labels=None):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.signals[idx], self.labels[idx]\n",
    "        return self.signals[idx]\n",
    "\n",
    "def normalize_signals(signals):\n",
    "    min_vals = signals.min(axis=1, keepdims=True)\n",
    "    max_vals = signals.max(axis=1, keepdims=True)\n",
    "    return (signals - min_vals) / (max_vals - min_vals + 1e-5)\n",
    "\n",
    "# --- STAGE 1: Building and Training Beat Classifier (Model A) on MIT-BIH Data ---\n",
    "print(\"\\n--- STAGE 1: Building and Training Beat Classifier (Model A) on MIT-BIH Data ---\")\n",
    "try:\n",
    "    mit_train_df = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "    mit_test_df = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "    mit_df = pd.concat([mit_train_df, mit_test_df], axis=0)\n",
    "    print(\"MIT-BIH dataset loaded successfully.\")\n",
    "    print(f\"Full MIT shape: {mit_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Make sure 'mitbih_train.csv' and 'mitbih_test.csv' are in the same folder.\")\n",
    "    print(\"If named differently (e.g., mitdb_*), rename them before running.\")\n",
    "    exit()\n",
    "\n",
    "# Process full MIT for training Model A (multi-class beat classification)\n",
    "# Note: Last column is label, first 187 are signals\n",
    "signals_a = mit_df.iloc[:, :-1].values.astype(np.float32)\n",
    "labels_a = mit_df.iloc[:, -1].values.astype(int)\n",
    "signals_a = normalize_signals(signals_a)\n",
    "\n",
    "# Split for train/val (80/20 on full data)\n",
    "X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(signals_a, labels_a, test_size=0.2, random_state=42, stratify=labels_a)\n",
    "\n",
    "# Tensors\n",
    "X_train_a_tensor = torch.tensor(X_train_a).unsqueeze(-1).to(device)\n",
    "X_val_a_tensor = torch.tensor(X_val_a).unsqueeze(-1).to(device)\n",
    "y_train_a_tensor = torch.tensor(y_train_a).to(device)\n",
    "y_val_a_tensor = torch.tensor(y_val_a).to(device)\n",
    "\n",
    "train_ds_a = ECGDataset(X_train_a_tensor, y_train_a_tensor)\n",
    "val_ds_a = ECGDataset(X_val_a_tensor, y_val_a_tensor)\n",
    "train_loader_a = DataLoader(train_ds_a, batch_size=32, shuffle=True)\n",
    "val_loader_a = DataLoader(val_ds_a, batch_size=32)\n",
    "\n",
    "# Model A: Multi-Class Beat Classifier (5 classes)\n",
    "model_a = SNN(input_size=1, hidden_size=256, output_size=5).to(device)\n",
    "optimizer_a = optim.Adam(model_a.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model_a.train()\n",
    "    total_loss = 0\n",
    "    for sig, lab in train_loader_a:\n",
    "        spk = model_a(sig)\n",
    "        rates = spk.sum(1)\n",
    "        loss = loss_fn(rates, lab)\n",
    "        optimizer_a.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_a.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Model A Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss / len(train_loader_a):.4f}\")\n",
    "\n",
    "    # Val\n",
    "    model_a.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    with torch.no_grad():\n",
    "        for sig, lab in val_loader_a:\n",
    "            spk = model_a(sig)\n",
    "            rates = spk.sum(1)\n",
    "            pred = torch.argmax(rates, dim=1)\n",
    "            val_preds.extend(pred.cpu().numpy())\n",
    "            val_true.extend(lab.cpu().numpy())\n",
    "    val_acc = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Model A Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Stage 1 Complete: Beat Classifier (Model A) Trained.\")\n",
    "print(classification_report(val_true, val_preds, target_names=['N', 'S', 'V', 'F', 'Q']))  # MIT classes\n",
    "\n",
    "# --- STAGE 2: Building and Training Anomaly Detector (Model B) on PTBDB Data ---\n",
    "print(\"\\n--- STAGE 2: Building and Training Anomaly Detector (Model B) on PTBDB Data ---\")\n",
    "try:\n",
    "    normal_df = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
    "    abnormal_df = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
    "    ptbdb_df = pd.concat([normal_df, abnormal_df], axis=0)\n",
    "    print(\"PTBDB dataset loaded successfully.\")\n",
    "    print(f\"Full PTBDB shape: {ptbdb_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Make sure 'ptbdb_normal.csv' and 'ptbdb_abnormal.csv' are in the folder.\")\n",
    "    exit()\n",
    "\n",
    "# Process PTBDB for binary anomaly detection\n",
    "signals_b = ptbdb_df.iloc[:, :-1].values.astype(np.float32)\n",
    "labels_b = ptbdb_df.iloc[:, -1].values.astype(int)\n",
    "signals_b = normalize_signals(signals_b)\n",
    "\n",
    "# Split\n",
    "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(signals_b, labels_b, test_size=0.2, random_state=42, stratify=labels_b)\n",
    "\n",
    "# Tensors\n",
    "X_train_b_tensor = torch.tensor(X_train_b).unsqueeze(-1).to(device)\n",
    "X_val_b_tensor = torch.tensor(X_val_b).unsqueeze(-1).to(device)\n",
    "y_train_b_tensor = torch.tensor(y_train_b).to(device)\n",
    "y_val_b_tensor = torch.tensor(y_val_b).to(device)\n",
    "\n",
    "train_ds_b = ECGDataset(X_train_b_tensor, y_train_b_tensor)\n",
    "val_ds_b = ECGDataset(X_val_b_tensor, y_val_b_tensor)\n",
    "train_loader_b = DataLoader(train_ds_b, batch_size=32, shuffle=True)\n",
    "val_loader_b = DataLoader(val_ds_b, batch_size=32)\n",
    "\n",
    "# Model B: Binary Anomaly Detector\n",
    "model_b = SNN(input_size=1, hidden_size=128, output_size=2).to(device)\n",
    "optimizer_b = optim.Adam(model_b.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_b.train()\n",
    "    total_loss = 0\n",
    "    for sig, lab in train_loader_b:\n",
    "        spk = model_b(sig)\n",
    "        rates = spk.sum(1)\n",
    "        loss = loss_fn(rates, lab)\n",
    "        optimizer_b.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_b.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Model B Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss / len(train_loader_b):.4f}\")\n",
    "\n",
    "    # Val\n",
    "    model_b.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    with torch.no_grad():\n",
    "        for sig, lab in val_loader_b:\n",
    "            spk = model_b(sig)\n",
    "            rates = spk.sum(1)\n",
    "            pred = torch.argmax(rates, dim=1)\n",
    "            val_preds.extend(pred.cpu().numpy())\n",
    "            val_true.extend(lab.cpu().numpy())\n",
    "    val_acc = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Model B Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Stage 2 Complete: Anomaly Detector (Model B) Trained.\")\n",
    "print(classification_report(val_true, val_preds, target_names=['Normal', 'Abnormal']))\n",
    "\n",
    "# --- OPTIONAL: Separate Inference on Original MIT Test (if needed for submission) ---\n",
    "print(\"\\n--- Inference on Original MIT-BIH Test Split using Model A ---\")\n",
    "# Reload test separately if needed\n",
    "mit_test_df = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "signals_test = mit_test_df.values.astype(np.float32)  # assuming no label in original test\n",
    "signals_test = normalize_signals(signals_test)\n",
    "signals_test_tensor = torch.tensor(signals_test).unsqueeze(-1).to(device)\n",
    "\n",
    "test_ds = ECGDataset(signals_test_tensor)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "model_a.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for sig in test_loader:\n",
    "        spk = model_a(sig)\n",
    "        rates = spk.sum(1)\n",
    "        pred = torch.argmax(rates, dim=1)\n",
    "        test_preds.extend(pred.cpu().numpy())\n",
    "print(\"Sample MIT Test Predictions (first 10):\", test_preds[:10])\n",
    "\n",
    "print(\"\\nAll Done! Models trained on respective datasets. For cascaded use: Apply Model B first to detect anomaly, then Model A for beat details if anomalous.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f9e693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in directory: c:\\Users\\saura\\Documents\\Major Project\\Implementing-SNN\n",
      "Using device: cpu\n",
      "\n",
      "--- Loading or Retraining Models to Save and Evaluate ---\n",
      "--- Handling Model A ---\n",
      "MIT-BIH train loaded.\n",
      "No saved Model A found. Training for 5 epochs to create and save...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 138\u001b[39m\n\u001b[32m    136\u001b[39m         loss = loss_fn_a(rates, lab)\n\u001b[32m    137\u001b[39m         optimizer_a.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m         optimizer_a.step()\n\u001b[32m    140\u001b[39m torch.save(model_a.state_dict(), model_a_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saura\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mSpikeFunction.backward\u001b[39m\u001b[34m(ctx, grad_output)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(ctx, grad_output):\n\u001b[32m     32\u001b[39m     mem_minus_th, = ctx.saved_tensors\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     surrogate = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmem_minus_th\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_output * surrogate, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "print(\"Working in directory:\", os.getcwd())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# SNN Classes (same as previous improved version)\n",
    "class SpikeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mem, threshold):\n",
    "        spike = (mem >= threshold).float()\n",
    "        ctx.save_for_backward(mem - threshold)\n",
    "        return spike\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        mem_minus_th, = ctx.saved_tensors\n",
    "        surrogate = torch.exp(- (mem_minus_th ** 2) / 0.1)\n",
    "        return grad_output * surrogate, None\n",
    "\n",
    "class LIF(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold=1.0, decay=0.95):\n",
    "        super(LIF, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.threshold = threshold\n",
    "        self.decay = decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        time_steps = x.size(1)\n",
    "        mem = torch.zeros(batch_size, self.fc.out_features, device=x.device)\n",
    "        spikes = []\n",
    "        for t in range(time_steps):\n",
    "            curr = self.fc(x[:, t, :])\n",
    "            mem = self.decay * mem + curr\n",
    "            spike = SpikeFunction.apply(mem, self.threshold)\n",
    "            mem = mem - spike * self.threshold\n",
    "            spikes.append(spike)\n",
    "        return torch.stack(spikes, dim=1)\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SNN, self).__init__()\n",
    "        self.lif1 = LIF(input_size, hidden_size1)\n",
    "        self.lif2 = LIF(hidden_size1, hidden_size2)\n",
    "        self.lif_out = LIF(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spk1 = self.lif1(x)\n",
    "        spk2 = self.lif2(spk1)\n",
    "        spk_out = self.lif_out(spk2)\n",
    "        return spk_out\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, signals, labels=None):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.signals[idx], self.labels[idx]\n",
    "        return self.signals[idx]\n",
    "\n",
    "def normalize_signals(signals):\n",
    "    min_vals = signals.min(axis=1, keepdims=True)\n",
    "    max_vals = signals.max(axis=1, keepdims=True)\n",
    "    return (signals - min_vals) / (max_vals - min_vals + 1e-5)\n",
    "\n",
    "# --- LOAD TRAINED MODELS (Assuming you have the .pth files from previous training) ---\n",
    "# If you trained in a previous session, save with: torch.save(model.state_dict(), 'model_a.pth')\n",
    "# Here, we'll assume you need to retrain briefly or load if exists. To dump/save, we'll train and save.\n",
    "\n",
    "print(\"\\n--- Loading or Retraining Models to Save and Evaluate ---\")\n",
    "\n",
    "# STAGE 1: Model A (Beat Classifier on MIT-BIH)\n",
    "print(\"--- Handling Model A ---\")\n",
    "try:\n",
    "    mit_train_df = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "    print(\"MIT-BIH train loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: File not found.\")\n",
    "    exit()\n",
    "\n",
    "signals_a = mit_train_df.iloc[:, :-1].values.astype(np.float32)\n",
    "labels_a = mit_train_df.iloc[:, -1].values.astype(int)\n",
    "signals_a = normalize_signals(signals_a)\n",
    "\n",
    "class_counts = Counter(labels_a)\n",
    "class_weights = torch.tensor([1.0 / class_counts[i] if class_counts[i] > 0 else 0 for i in range(5)], dtype=torch.float).to(device)\n",
    "loss_fn_a = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(signals_a, labels_a, test_size=0.2, random_state=42, stratify=labels_a)\n",
    "X_train_a_tensor = torch.tensor(X_train_a).unsqueeze(-1).to(device)\n",
    "X_val_a_tensor = torch.tensor(X_val_a).unsqueeze(-1).to(device)\n",
    "y_train_a_tensor = torch.tensor(y_train_a).to(device)\n",
    "y_val_a_tensor = torch.tensor(y_val_a).to(device)\n",
    "\n",
    "train_ds_a = ECGDataset(X_train_a_tensor, y_train_a_tensor)\n",
    "val_ds_a = ECGDataset(X_val_a_tensor, y_val_a_tensor)\n",
    "train_loader_a = DataLoader(train_ds_a, batch_size=64, shuffle=True)\n",
    "val_loader_a = DataLoader(val_ds_a, batch_size=64)\n",
    "\n",
    "model_a = SNN(input_size=1, hidden_size1=256, hidden_size2=128, output_size=5).to(device)\n",
    "\n",
    "# Load if exists, else train briefly (5 epochs to simulate, adjust if you have saved weights)\n",
    "model_a_path = 'model_a.pth'\n",
    "if os.path.exists(model_a_path):\n",
    "    model_a.load_state_dict(torch.load(model_a_path))\n",
    "    print(\"Loaded Model A from disk.\")\n",
    "else:\n",
    "    print(\"No saved Model A found. Training for 5 epochs to create and save...\")\n",
    "    optimizer_a = optim.Adam(model_a.parameters(), lr=0.0005)\n",
    "    for epoch in range(5):\n",
    "        model_a.train()\n",
    "        for sig, lab in train_loader_a:\n",
    "            spk = model_a(sig)\n",
    "            rates = spk.sum(1)\n",
    "            loss = loss_fn_a(rates, lab)\n",
    "            optimizer_a.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_a.step()\n",
    "    torch.save(model_a.state_dict(), model_a_path)\n",
    "    print(\"Trained briefly and saved Model A to 'model_a.pth'\")\n",
    "\n",
    "# Evaluate Model A on val\n",
    "model_a.eval()\n",
    "val_preds_a = []\n",
    "val_true_a = []\n",
    "with torch.no_grad():\n",
    "    for sig, lab in val_loader_a:\n",
    "        spk = model_a(sig)\n",
    "        rates = spk.sum(1)\n",
    "        pred = torch.argmax(rates, dim=1)\n",
    "        val_preds_a.extend(pred.cpu().numpy())\n",
    "        val_true_a.extend(lab.cpu().numpy())\n",
    "acc_a = accuracy_score(val_true_a, val_preds_a)\n",
    "print(f\"Model A Val Accuracy: {acc_a:.4f}\")\n",
    "print(classification_report(val_true_a, val_preds_a, target_names=['N', 'S', 'V', 'F', 'Q'], zero_division=0))\n",
    "\n",
    "# Confusion Matrix for A\n",
    "cm_a = confusion_matrix(val_true_a, val_preds_a)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_a, annot=True, fmt='d', cmap='Blues', xticklabels=['N', 'S', 'V', 'F', 'Q'], yticklabels=['N', 'S', 'V', 'F', 'Q'])\n",
    "plt.title('Model A Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('model_a_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy bar\n",
    "class_acc_a = cm_a.diagonal() / cm_a.sum(axis=1)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=['N', 'S', 'V', 'F', 'Q'], y=class_acc_a)\n",
    "plt.title('Model A Per-Class Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig('model_a_per_class_acc.png')\n",
    "plt.show()\n",
    "\n",
    "# STAGE 2: Model B (Anomaly Detector on PTBDB)\n",
    "print(\"\\n--- Handling Model B ---\")\n",
    "try:\n",
    "    normal_df = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
    "    abnormal_df = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
    "    print(\"PTBDB loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: File not found.\")\n",
    "    exit()\n",
    "\n",
    "signals_b = np.vstack((normal_df.iloc[:, :-1].values, abnormal_df.iloc[:, :-1].values)).astype(np.float32)\n",
    "labels_b = np.hstack((normal_df.iloc[:, -1].values, abnormal_df.iloc[:, -1].values)).astype(int)\n",
    "signals_b = normalize_signals(signals_b)\n",
    "\n",
    "class_counts_b = Counter(labels_b)\n",
    "class_weights_b = torch.tensor([1.0 / class_counts_b[i] for i in range(2)], dtype=torch.float).to(device)\n",
    "loss_fn_b = nn.CrossEntropyLoss(weight=class_weights_b)\n",
    "\n",
    "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(signals_b, labels_b, test_size=0.2, random_state=42, stratify=labels_b)\n",
    "X_train_b_tensor = torch.tensor(X_train_b).unsqueeze(-1).to(device)\n",
    "X_val_b_tensor = torch.tensor(X_val_b).unsqueeze(-1).to(device)\n",
    "y_train_b_tensor = torch.tensor(y_train_b).to(device)\n",
    "y_val_b_tensor = torch.tensor(y_val_b).to(device)\n",
    "\n",
    "train_ds_b = ECGDataset(X_train_b_tensor, y_train_b_tensor)\n",
    "val_ds_b = ECGDataset(X_val_b_tensor, y_val_b_tensor)\n",
    "train_loader_b = DataLoader(train_ds_b, batch_size=64, shuffle=True)\n",
    "val_loader_b = DataLoader(val_ds_b, batch_size=64)\n",
    "\n",
    "model_b = SNN(input_size=1, hidden_size1=256, hidden_size2=128, output_size=2).to(device)\n",
    "\n",
    "model_b_path = 'model_b.pth'\n",
    "if os.path.exists(model_b_path):\n",
    "    model_b.load_state_dict(torch.load(model_b_path))\n",
    "    print(\"Loaded Model B from disk.\")\n",
    "else:\n",
    "    print(\"No saved Model B found. Training for 5 epochs to create and save...\")\n",
    "    optimizer_b = optim.Adam(model_b.parameters(), lr=0.0005)\n",
    "    for epoch in range(5):\n",
    "        model_b.train()\n",
    "        for sig, lab in train_loader_b:\n",
    "            spk = model_b(sig)\n",
    "            rates = spk.sum(1)\n",
    "            loss = loss_fn_b(rates, lab)\n",
    "            optimizer_b.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_b.step()\n",
    "    torch.save(model_b.state_dict(), model_b_path)\n",
    "    print(\"Trained briefly and saved Model B to 'model_b.pth'\")\n",
    "\n",
    "# Evaluate Model B on val\n",
    "model_b.eval()\n",
    "val_preds_b = []\n",
    "val_true_b = []\n",
    "with torch.no_grad():\n",
    "    for sig, lab in val_loader_b:\n",
    "        spk = model_b(sig)\n",
    "        rates = spk.sum(1)\n",
    "        pred = torch.argmax(rates, dim=1)\n",
    "        val_preds_b.extend(pred.cpu().numpy())\n",
    "        val_true_b.extend(lab.cpu().numpy())\n",
    "acc_b = accuracy_score(val_true_b, val_preds_b)\n",
    "print(f\"Model B Val Accuracy: {acc_b:.4f}\")\n",
    "print(classification_report(val_true_b, val_preds_b, target_names=['Normal', 'Abnormal'], zero_division=0))\n",
    "\n",
    "# Confusion Matrix for B\n",
    "cm_b = confusion_matrix(val_true_b, val_preds_b)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_b, annot=True, fmt='d', cmap='Greens', xticklabels=['Normal', 'Abnormal'], yticklabels=['Normal', 'Abnormal'])\n",
    "plt.title('Model B Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('model_b_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy bar for B\n",
    "class_acc_b = cm_b.diagonal() / cm_b.sum(axis=1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=['Normal', 'Abnormal'], y=class_acc_b)\n",
    "plt.title('Model B Per-Class Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig('model_b_per_class_acc.png')\n",
    "plt.show()\n",
    "\n",
    "# --- Dump More Metrics to Files ---\n",
    "metrics_a = {\n",
    "    'accuracy': acc_a,\n",
    "    'classification_report': classification_report(val_true_a, val_preds_a, output_dict=True, zero_division=0)\n",
    "}\n",
    "with open('model_a_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_a, f, indent=4)\n",
    "\n",
    "metrics_b = {\n",
    "    'accuracy': acc_b,\n",
    "    'classification_report': classification_report(val_true_b, val_preds_b, output_dict=True, zero_division=0)\n",
    "}\n",
    "with open('model_b_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_b, f, indent=4)\n",
    "\n",
    "print(\"\\nModels dumped to 'model_a.pth' and 'model_b.pth'.\")\n",
    "print(\"Graphs saved as PNGs: confusion matrices and per-class acc.\")\n",
    "print(\"Metrics dumped to JSON files.\")\n",
    "print(\"To load later: model.load_state_dict(torch.load('path.pth'))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c635ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
